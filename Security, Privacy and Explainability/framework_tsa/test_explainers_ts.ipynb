{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 21:54:48.097231: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import (\n",
    "    Dense,\n",
    "   # Dropout,\n",
    "   # Flatten,\n",
    "   # Input,\n",
    "   # concatenate,\n",
    ")\n",
    "# from keras.layers import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add framework_tsa to path\n",
    "sys.path.append('../framework_tsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuri/anaconda3/envs/hlt/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ffnn_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to predict new values of kpi \n",
    "\n",
    "\"\"\"### FFNN model\"\"\"\n",
    "\n",
    "def forecast_with_ffnn(\n",
    "    time_series, n_lags=14, hidden_units_options=[32, 64, 128], epochs=50, batch_size=16, seed=0, plot_results=True\n",
    "):\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Make sure the time series is ordered and has no missing values\n",
    "    time_series = time_series.dropna()\n",
    "    time_series = time_series.sort_index()\n",
    "\n",
    "    # Split the time series into training, validation, and test sets\n",
    "    train_size = int(len(time_series) * 0.7)\n",
    "    val_size = int(len(time_series) * 0.2)\n",
    "    test_size = len(time_series) - train_size - val_size\n",
    "\n",
    "    train = time_series.iloc[:train_size]\n",
    "    validation = time_series.iloc[train_size:train_size + val_size]\n",
    "    test = time_series.iloc[train_size + val_size:]\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    validation_scaled = scaler.transform(validation)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    train_val_scaled = np.vstack([train_scaled, validation_scaled])\n",
    "\n",
    "    # Prepare the data\n",
    "    def prepare_data(series, n_lags):\n",
    "        series = np.array(series)\n",
    "        X = np.lib.stride_tricks.sliding_window_view(series, window_shape=n_lags)[:-1]\n",
    "        y = series[n_lags:]\n",
    "        return X, y\n",
    "\n",
    "    X_train_val, y_train_val = prepare_data(train_val_scaled.flatten(), n_lags)\n",
    "    X_test, y_test = prepare_data(test_scaled.flatten(), n_lags)\n",
    "\n",
    "    # Split train and validation\n",
    "    split_idx = len(train)\n",
    "    X_train, y_train = X_train_val[:split_idx-n_lags], y_train_val[:split_idx-n_lags]\n",
    "    X_val, y_val = X_train_val[split_idx-n_lags:], y_train_val[split_idx-n_lags:]\n",
    "\n",
    "    # Reshape data\n",
    "    X_train = X_train.reshape(X_train.shape[0], n_lags)\n",
    "    X_val = X_val.reshape(X_val.shape[0], n_lags)\n",
    "    X_test = X_test.reshape(X_test.shape[0], n_lags)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    best_rmse = float('inf')\n",
    "    best_units = None\n",
    "    for units in hidden_units_options:\n",
    "        model = Sequential([\n",
    "            Dense(units, activation='relu', input_dim=n_lags),\n",
    "            Dense(units // 2, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        y_val_pred = model.predict(X_val).ravel()\n",
    "        y_val_rescaled = scaler.inverse_transform(y_val.reshape(-1, 1)).ravel()\n",
    "        y_val_pred_rescaled = scaler.inverse_transform(y_val_pred.reshape(-1, 1)).ravel()\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_rescaled, y_val_pred_rescaled))\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_units = units\n",
    "\n",
    "    # Final model training\n",
    "    model = Sequential([\n",
    "        Dense(best_units, activation='relu', input_dim=n_lags),\n",
    "        Dense(best_units // 2, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train_val, y_train_val, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # Predictions\n",
    "    y_test_pred = model.predict(X_test).ravel()\n",
    "    y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "    y_test_pred_rescaled = scaler.inverse_transform(y_test_pred.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Sliding window predictions\n",
    "    predictions = []\n",
    "    input_seq = X_test[0]\n",
    "    for _ in range(len(test)):\n",
    "        pred = model.predict(input_seq.reshape(1, -1), verbose=0)[0, 0]\n",
    "        predictions.append(pred)\n",
    "        input_seq = np.append(input_seq[1:], pred)\n",
    "\n",
    "    predictions_rescaled = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Residuals and confidence interval\n",
    "    residuals = y_test_rescaled.flatten() - y_test_pred_rescaled\n",
    "    std_residuals = np.std(residuals)\n",
    "    confidence_interval = (predictions_rescaled - 2 * std_residuals, predictions_rescaled + 2 * std_residuals)\n",
    "\n",
    "    # Plot results\n",
    "    if plot_results:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(pd.to_datetime(time_series.index), time_series['value'], label='Full Series', color='gray', linestyle='--', alpha=0.5)\n",
    "        plt.plot(pd.to_datetime(train.index), train['value'], label='Train', color='blue')\n",
    "        plt.plot(pd.to_datetime(validation.index), validation['value'], label='Validation', color='green')\n",
    "        plt.plot(pd.to_datetime(test.index), test['value'], label='Test', color='red')\n",
    "        plt.plot(pd.to_datetime(test.index), predictions_rescaled, label='Forecast', color='orange')\n",
    "        plt.fill_between(\n",
    "            test.index,\n",
    "            predictions_rescaled - 2 * std_residuals,\n",
    "            predictions_rescaled + 2 * std_residuals,\n",
    "            color='orange', alpha=0.2, label='Confidence Interval'\n",
    "        )\n",
    "        plt.title('FFNN Forecast with Train, Validation, Test, and Confidence Interval')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Consumption')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../smart_app_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>kpi</th>\n",
       "      <th>col</th>\n",
       "      <th>selected_f</th>\n",
       "      <th>unusable_data_bool</th>\n",
       "      <th>inconsistencies_flag</th>\n",
       "      <th>h_trend_drift</th>\n",
       "      <th>best_decomposition</th>\n",
       "      <th>count_miss_values</th>\n",
       "      <th>time_series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Large Capacity Cutting Machine 1</td>\n",
       "      <td>working_time</td>\n",
       "      <td>sum</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.362321</td>\n",
       "      <td>{'trend': [0.14130069905930454, 0.829459955311...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.6933152988336704, 0.9371253794295603, 0.133...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Large Capacity Cutting Machine 1</td>\n",
       "      <td>working_time</td>\n",
       "      <td>avg</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.110392</td>\n",
       "      <td>{'trend': [0.5609923583193656, 0.7911506155336...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.6268468606234581, 0.7732760976448547, 0.012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Large Capacity Cutting Machine 1</td>\n",
       "      <td>working_time</td>\n",
       "      <td>min</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.775245</td>\n",
       "      <td>{'trend': [0.27097451371627557, 0.826777486751...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.7130808414504867, 0.41865744438545405, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Large Capacity Cutting Machine 1</td>\n",
       "      <td>working_time</td>\n",
       "      <td>max</td>\n",
       "      <td>30</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.841182</td>\n",
       "      <td>{'trend': [0.9349481961370171, 0.1643700942223...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.15051081303471137, 0.2611618488050198, 0.21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large Capacity Cutting Machine 1</td>\n",
       "      <td>idle_time</td>\n",
       "      <td>sum</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.803067</td>\n",
       "      <td>{'trend': [0.21443884861264562, 0.325835368012...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.9629539288049326, 0.48448134042859514, 0.73...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name           kpi  col  selected_f  \\\n",
       "0  Large Capacity Cutting Machine 1  working_time  sum           7   \n",
       "1  Large Capacity Cutting Machine 1  working_time  avg           7   \n",
       "2  Large Capacity Cutting Machine 1  working_time  min          30   \n",
       "3  Large Capacity Cutting Machine 1  working_time  max          30   \n",
       "4  Large Capacity Cutting Machine 1     idle_time  sum           7   \n",
       "\n",
       "   unusable_data_bool  inconsistencies_flag  h_trend_drift  \\\n",
       "0               False                 False       0.362321   \n",
       "1                True                 False      -1.110392   \n",
       "2               False                  True      -0.775245   \n",
       "3                True                 False       0.841182   \n",
       "4                True                  True      -0.803067   \n",
       "\n",
       "                                  best_decomposition  \\\n",
       "0  {'trend': [0.14130069905930454, 0.829459955311...   \n",
       "1  {'trend': [0.5609923583193656, 0.7911506155336...   \n",
       "2  {'trend': [0.27097451371627557, 0.826777486751...   \n",
       "3  {'trend': [0.9349481961370171, 0.1643700942223...   \n",
       "4  {'trend': [0.21443884861264562, 0.325835368012...   \n",
       "\n",
       "                                   count_miss_values  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                         time_series  \n",
       "0  [0.6933152988336704, 0.9371253794295603, 0.133...  \n",
       "1  [0.6268468606234581, 0.7732760976448547, 0.012...  \n",
       "2  [0.7130808414504867, 0.41865744438545405, 0.05...  \n",
       "3  [0.15051081303471137, 0.2611618488050198, 0.21...  \n",
       "4  [0.9629539288049326, 0.48448134042859514, 0.73...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creation of offline feature store, called FEAT_DATA\n",
    "\n",
    "# Valori unici per ogni colonna\n",
    "names = data['name'].unique()\n",
    "kpis = data['kpi'].unique()\n",
    "cols = ['sum', 'avg', 'min', 'max']\n",
    "\n",
    "# Genera tutte le combinazioni possibili\n",
    "combinations = list(product(names, kpis, cols))\n",
    "\n",
    "# Crea un DataFrame a partire dalle combinazioni\n",
    "feat_data = pd.DataFrame(combinations, columns=['name', 'kpi', 'col'])\n",
    "\n",
    "# Aggiungi le altre colonne con valori casuali\n",
    "feat_data['selected_f'] = np.random.choice([7, 30], len(feat_data))\n",
    "feat_data['unusable_data_bool'] = np.random.choice([True, False], len(feat_data))\n",
    "feat_data['inconsistencies_flag'] = np.random.choice([True, False], len(feat_data))\n",
    "feat_data['h_trend_drift'] = np.random.normal(0, 1, len(feat_data))\n",
    "\n",
    "decompositions = []\n",
    "for _ in range(len(feat_data)):\n",
    "    trend = np.random.rand(233)  # Trend fittizio\n",
    "    seasonal = np.random.rand(233)  # Seasonal fittizio\n",
    "    residuals = np.random.rand(233)  # Residuals fittizio\n",
    "    decompositions.append({\n",
    "        'trend': trend,\n",
    "        'seasonal': seasonal,\n",
    "        'residuals': residuals\n",
    "    })\n",
    "feat_data['best_decomposition'] = decompositions\n",
    "\n",
    "# Aggiungi la colonna count_miss_values con vettori di 10 zeri\n",
    "feat_data['count_miss_values'] = [np.zeros(10).tolist() for _ in range(len(feat_data))]\n",
    "feat_data['time_series'] = [np.random.rand(233) for _ in range(len(feat_data))]\n",
    "\n",
    "# Mostra le prime righe del DataFrame\n",
    "feat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 21:54:55.944821: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extracting the time_series for each combination\n",
    "for name in feat_data['name'].unique():\n",
    "    for k in feat_data['kpi'].unique():\n",
    "        for col in feat_data['col'].unique():\n",
    "            # Filter the dataframe to get the corresponding row\n",
    "            time_series = feat_data[(feat_data['name'] == name) &\n",
    "                                   (feat_data['kpi'] == k) &\n",
    "                                   (feat_data['col'] == col)]['time_series']\n",
    "            time_series = pd.DataFrame({'value': time_series.tolist()[0]}, index = data['time'].unique())\n",
    "\n",
    "            # Pass the time_series to the detect anomalies function\n",
    "            trained_ffnn, _ = forecast_with_ffnn(time_series, plot_results=False)\n",
    "            break\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Since axis is `None`, must provide window_shape for all dimensions of `x`; got 1 window_shape elements and `x.ndim` is 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m explainer \u001b[38;5;241m=\u001b[39m ffnn_explainer\u001b[38;5;241m.\u001b[39mget_regression_explainer(trained_ffnn, time_series)\n",
      "File \u001b[0;32m~/myGitShit/proactive-AI-industry-5.0/Security, Privacy and Explainability/framework_tsa/ffnn_explainer.py:19\u001b[0m, in \u001b[0;36mget_regression_explainer\u001b[0;34m(model, time_series, n_lags)\u001b[0m\n\u001b[1;32m     16\u001b[0m ts \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(ts)\n\u001b[1;32m     18\u001b[0m ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(ts)\n\u001b[0;32m---> 19\u001b[0m ts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mstride_tricks\u001b[38;5;241m.\u001b[39msliding_window_view(ts, window_shape\u001b[38;5;241m=\u001b[39mn_lags)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m ts \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mreshape(ts\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], n_lags)\n\u001b[1;32m     21\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(model, ts)\n",
      "File \u001b[0;32m~/anaconda3/envs/hlt/lib/python3.11/site-packages/numpy/lib/stride_tricks.py:315\u001b[0m, in \u001b[0;36msliding_window_view\u001b[0;34m(x, window_shape, axis, subok, writeable)\u001b[0m\n\u001b[1;32m    313\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mndim))\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(window_shape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(axis):\n\u001b[0;32m--> 315\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSince axis is `None`, must provide \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    316\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindow_shape for all dimensions of `x`; \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    317\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(window_shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m window_shape elements \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    318\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand `x.ndim` is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     axis \u001b[38;5;241m=\u001b[39m normalize_axis_tuple(axis, x\u001b[38;5;241m.\u001b[39mndim, allow_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Since axis is `None`, must provide window_shape for all dimensions of `x`; got 1 window_shape elements and `x.ndim` is 2."
     ]
    }
   ],
   "source": [
    "explainer = ffnn_explainer.get_regression_explainer(trained_ffnn, time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"### Prediction of future values\n",
    "\n",
    "Here we use the trained model to predict the future values \\\n",
    "The function is called once a day when we receive the new data, and the new predictions will substitute the previous ones\n",
    "\"\"\"\n",
    "\n",
    "def forecast_future(time_series, trained_ffnn, scaler, n_future = 21, n_lags = 14, plot_results=True):\n",
    "    \"\"\"\n",
    "    Forecast future values of the time series using the trained FFNN model.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series (pd.Series): The input time series.\n",
    "    - trained_ffnn (tensorflow.keras.models.Sequential): The trained FFNN model.\n",
    "    - scaler (sklearn.preprocessing.MinMaxScaler): The scaler used to scale the data.\n",
    "    - n_future (int): The number of future values to predict.\n",
    "    - n_lags (int): The number of lags (time window) for the prediction. N.B. must be the same of training function\n",
    "    - plot_results (bool): Whether to plot the results.\n",
    "\n",
    "    Returns:\n",
    "    - predictions_rescaled (numpy.array): An array containing the future predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(time_series) >= n_lags:\n",
    "\n",
    "      # Extract the last sequence of n_lags from the time series\n",
    "      last_sequence = time_series[-n_lags:].values.reshape(1, -1)  # Last n_lags values\n",
    "\n",
    "      predictions = []\n",
    "      input_seq = last_sequence.flatten()\n",
    "\n",
    "      # Standard deviation of residuals for uncertainty\n",
    "      residuals = time_series[-n_lags:].values - trained_ffnn.predict(input_seq.reshape(1, -1), verbose=0).flatten()\n",
    "      std_residuals = np.std(residuals)\n",
    "\n",
    "      # Predict the future values\n",
    "      for _ in range(n_future):\n",
    "          # Predict the next value\n",
    "          pred = trained_ffnn.predict(input_seq.reshape(1, -1), verbose=0)[0, 0]\n",
    "          predictions.append(pred)\n",
    "\n",
    "          # Add the prediction to the input sequence for the next step\n",
    "          input_seq = np.append(input_seq[1:], pred)\n",
    "\n",
    "      # Rescale the predictions back to the original scale\n",
    "      predictions_rescaled = scaler.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "\n",
    "      # Confidence interval (± 2 standard deviations)\n",
    "      confidence_interval_lower = predictions_rescaled - 2 * std_residuals\n",
    "      confidence_interval_upper = predictions_rescaled + 2 * std_residuals\n",
    "\n",
    "      # Plot the results\n",
    "      if plot_results:\n",
    "          plt.figure(figsize=(8, 4))\n",
    "          plt.plot(pd.to_datetime(time_series.index), time_series.values, label='Original Series', color='gray', linestyle='--', alpha=0.5)\n",
    "          future_dates = pd.date_range(pd.to_datetime(time_series.index[-1]), periods=n_future + 1, freq='D')[1:]\n",
    "          plt.plot(future_dates, predictions_rescaled, label='Forecast', color='orange')\n",
    "          plt.fill_between(future_dates, confidence_interval_lower, confidence_interval_upper,\n",
    "                          color='orange', alpha=0.2, label='Confidence Interval')\n",
    "          plt.title('FFNN Forecast with Confidence Interval')\n",
    "          plt.xlabel('Time')\n",
    "          plt.ylabel('Value')\n",
    "          plt.legend()\n",
    "          plt.grid()\n",
    "          plt.show()\n",
    "    else:\n",
    "      print('Time series length not sufficient to perform prediction')\n",
    "\n",
    "    return predictions_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_416015/2156914593.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtime_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/hlt/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5898\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5899\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_list'"
     ]
    }
   ],
   "source": [
    "time_series.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value    0.478933\n",
       "Name: 2024-03-16T00:00:00Z, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.iloc[15,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ffnn_explainer' has no attribute 'get_explanation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sv \u001b[38;5;241m=\u001b[39m ffnn_explainer\u001b[38;5;241m.\u001b[39mget_explanation(explainer, time_series[\u001b[38;5;241m15\u001b[39m,:])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ffnn_explainer' has no attribute 'get_explanation'"
     ]
    }
   ],
   "source": [
    "sv = ffnn_explainer.get_explanation(explainer, time_series[15,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
